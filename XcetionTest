# import necessary libraries
# model build imports
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import matplotlib
matplotlib.use('Agg')
from matplotlib import pyplot as plt
from matplotlib import image as mpimg
import numpy as np
import json
from numpy import *
from skimage import io, color ,filters, transform
import csv
from io import StringIO

from keras.models import Sequential, Model
from keras.layers import Input, GlobalAveragePooling2D, Dense, Reshape, Dropout, Concatenate, Conv2D
import keras.losses, keras.optimizers, keras.metrics, keras.preprocessing.image, keras.applications
from keras.regularizers import l2
import keras_tuner
from kerastuner.tuners import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters

from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers, applications
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping
from keras import backend as K
import tensorflow as tf
from PIL import Image
import Augmentor
import shutil
from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc, confusion_matrix

#setting the gpu
gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(gpus[0], True)
tf.config.experimental_run_functions_eagerly(True)

# model architecture
'''
    The model works by the idea that of the separable convolution for the parameters pooling from the image.
    My understanding, as we go through more max pooling, by this technique and generally the model works on extracting
    the most features by filters, not like in the high detailed start with about 128 filters and more.

# creating the Conv-Batch Norm block

def conv_bn(x, filters, kernel_size, strides=1):
    x = Conv2D(filters=filters,
               kernel_size=kernel_size,
               strides=strides,
               padding='same',
               use_bias=False)(x)
    x = BatchNormalization()(x)


    return x


# creating separableConv-Batch Norm block

def sep_bn(x, filters, kernel_size, strides=1):
    x = SeparableConv2D(filters=filters,
                        kernel_size=kernel_size,
                        strides=strides,
                        padding='same',
                        use_bias=False)(x)
    x = BatchNormalization()(x)


    return x


# entry flow
def entry_flow(x):
    x = conv_bn(x, filters=32, kernel_size=3, strides=2)
    x = ReLU()(x)
    x = conv_bn(x, filters=64, kernel_size=3, strides=1)
    tensor = ReLU()(x)

    x = sep_bn(tensor, filters=128, kernel_size=3)
    x = ReLU()(x)
    x = sep_bn(x, filters=128, kernel_size=3)
    x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)

    tensor = conv_bn(tensor, filters=128, kernel_size=1, strides=2)
    x = Add()([tensor, x])

    x = ReLU()(x)
    x = sep_bn(x, filters=256, kernel_size=3)
    x = ReLU()(x)
    x = sep_bn(x, filters=256, kernel_size=3)
    x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)

    tensor = conv_bn(tensor, filters=256, kernel_size=1, strides=2)
    x = Add()([tensor, x])

    x = ReLU()(x)
    x = sep_bn(x, filters=728, kernel_size=3)
    x = ReLU()(x)
    x = sep_bn(x, filters=728, kernel_size=3)
    x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)

    tensor = conv_bn(tensor, filters=728, kernel_size=1, strides=2)
    x = Add()([tensor, x])


    return x


# middle flow

def middle_flow(tensor):
    for _ in range(8):
        x = ReLU()(tensor)
        x = sep_bn(x, filters=728, kernel_size=3)
        x = ReLU()(x)
        x = sep_bn(x, filters=728, kernel_size=3)
        x = ReLU()(x)
        x = sep_bn(x, filters=728, kernel_size=3)
        x = ReLU()(x)
        tensor = Add()([tensor, x])

    return tensor


# exit flow
def exit_flow(tensor):
    x = ReLU()(tensor)
    x = sep_bn(x, filters=728, kernel_size=3)
    x = ReLU()(x)
    x = sep_bn(x, filters=1024, kernel_size=3)
    x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)

    tensor = conv_bn(tensor, filters=1024, kernel_size=1, strides=2)
    x = Add()([tensor, x])

    x = sep_bn(x, filters=1536, kernel_size=3)
    x = ReLU()(x)
    x = sep_bn(x, filters=2048, kernel_size=3)
    x = GlobalAvgPool2D()(x)

    x = Dense(units=1, activation='sigmoid')(x)

    return x

# model code

input = Input(shape=(299, 299, 3))
x = entry_flow(input)
x = middle_flow(x)
output = exit_flow(x)

model = Model(inputs=input, outputs=output)
'''







# downlaoding and prepering the dataset when the labels are in text or/also the dataset iamges and labels are not orgenized in the file

'''
we need to specify the data to classes of the kinds of objects we check, so we need when running through the dataset 
of images to know if the image is from what class, so we will check in the label for the same name or properties 
(in this code for name) and check it for its kind (what is the object suppose to be presented as) and then by knowing 
its class we can put them into their own folder for the model to work with later

input_dir = "/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/ER"
target_dir = "/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Labels/Labels_ER"
def Labeling_operations(target_dir_path):
    #geting the H-SCORE specified in the CSV file for every tiles for every pietient
    patients_information = []
    patients_order = []
    for file in os.listdir(target_dir):
        file_path = os.path.join(target_dir, file)
        with open(file_path, mode='r', newline='') as file:
            csv_reader = csv.DictReader(file)
            data = []
            image_name_per_patient = []
            for row in csv_reader:
                if row[' manual_annot'] != '0':
                    image_name_per_patient.append((row['file_image'][:len(row['file_image']) - 3] + "jpg").replace('/','\\'))
                    info_temp = row[None]
                    info_temp.append(row[' avgDABnuclei_IHC'])
                    data.append(info_temp)
                else:
                    data.append(None)
                    image_name_per_patient.append(None)
            patients_information.append(data)
            patients_order.append(image_name_per_patient)

            # classifing all the nuclei in every image by its H-score to the class of ER posotivity it belongs to - 0,1,
            # 2 or 3 using the treshholds
            ER_thresh_hold = [0.06, 0.26, 0.46]
            for paitient_number in range(len(patients_information)):
                for image_number in range(len(patients_information[paitient_number])):
                    if patients_information[paitient_number][image_number] != None:
                        for nuclei_number in range(len(patients_information[paitient_number][image_number])):
                            if float(patients_information[paitient_number][image_number][nuclei_number]) < ER_thresh_hold[0]:
                                patients_information[paitient_number][image_number][nuclei_number] = 0
                            elif ER_thresh_hold[0] < float(patients_information[paitient_number][image_number][nuclei_number]) < ER_thresh_hold[1]:
                                patients_information[paitient_number][image_number][nuclei_number] = 1
                            elif ER_thresh_hold[1] < float(patients_information[paitient_number][image_number][nuclei_number]) < ER_thresh_hold[2]:
                                patients_information[paitient_number][image_number][nuclei_number] = 2
                            else:
                                patients_information[paitient_number][image_number][nuclei_number] = 3

    #making the precantage of ER positive cells from the whole number of cells per image
    positive_precentages = []
    for paitient_number in range(len(patients_information)):
        patient_precentage = []
        for image_number in range(len(patients_information[paitient_number])):
            if patients_information[paitient_number][image_number] is not None:
                total_cells_count = patients_information[paitient_number][image_number].count(0) + patients_information[paitient_number][image_number].count(1) + patients_information[paitient_number][image_number].count(2) + patients_information[paitient_number][image_number].count(3)
                positive_cells = patients_information[paitient_number][image_number].count(1) + patients_information[paitient_number][image_number].count(2) + patients_information[paitient_number][image_number].count(3)
                positive_precentage = positive_cells/total_cells_count
                patient_precentage.append(positive_precentage)
            else:
                patient_precentage.append(-1)
        positive_precentages.append(patient_precentage)

    # putting those precentages into a list of labels based on the positive cells precantage that was provided into 4
    # classes: 1 - ER negative (< 1%), 2 - Weakly ER-Positive (1% to 10%), 3 - Moderately ER-Positive (10% to 30%),
    # 4 - Strongly ER-Positive (> 30%)
    patients_images_labels = []
    for patient in positive_precentages:
        patient_images_labels = []
        for p in patient:
            if p != -1:
                new_p = p*100
                if new_p < 1:
                    patient_images_labels.append(1)
                elif 1 <= new_p < 10:
                    patient_images_labels.append(2)
                elif 10 <= new_p < 30:
                    patient_images_labels.append(3)
                else:
                    patient_images_labels.append(4)
            else:
                patient_images_labels.append(None)
        patients_images_labels.append(patient_images_labels)

    return (patients_images_labels, patients_order)

patients_images_labels = Labeling_operations(target_dir)[0] #callign the label creation function from processing the information CVS file format
patients_order = Labeling_operations(target_dir)[1]








#getting all the images that are good for use based on the annotations we made in the "patients_images_labels" list
patients_imaging_data = []
for patient in patients_order:
    patient_imaging_data = []
    for image_name in patient:
        if image_name != None:
            if os.path.exists(os.path.join(input_dir, image_name.replace('\\', '/'))):
                patient_imaging_data.append(image_name)
    patients_imaging_data.append(patient_imaging_data)

#filtering the data - as the data contains of a lott of small details, the most filtering was throwing out images with small number of information
# this specific filtering we are using the Otsu method in order to calculate better the kind of pixle colors and thier
# distrebution to detect the background of the slides in the image tiles, therefor deleting it if its too big
# reletavilly to the image size
for patient in patients_imaging_data:
    for image in patient:
        if patients_images_labels[patients_imaging_data.index(patient)][patient.index(image)] != None:

            # Load the image using PIL
            opened_image = Image.open(os.path.join(input_dir, image.replace('\\', '/')))

            # Convert the image to a NumPy array
            image_array = np.array(opened_image)

            # Define the lower and upper bounds for the color range
            lower_bound = np.array([230, 230, 230], dtype=np.uint8)
            upper_bound = np.array([255, 255, 255], dtype=np.uint8)

            # Create a mask that identifies pixels within the specified color range
            mask = np.all((image_array >= lower_bound) & (image_array <= upper_bound), axis=-1)

            # Calculate the percentage of pixels within the specified color range
            total_pixels = mask.size
            matching_pixels = np.count_nonzero(mask)
            percentage = (matching_pixels / total_pixels) * 100

            if percentage > 70:
                patients_images_labels[patients_imaging_data.index(patient)].pop(patient.index(image))
                index = patients_imaging_data.index(patient)
                patients_imaging_data[patients_imaging_data.index(patient)].remove(image)

def folder_sorting(list_of_cuts, saving_path):
    ERpositive_class_objects = []
    ERnegative_class_objects = []
    ERpositive_class_object_counter = 0
    ERnegative_class_object_counter = 0


    parts_path_name = ["training_part", "validation_part", "testing_part"]
    class_names = ["ER+", "ER-"]
    for dir in parts_path_name:
        using_part = os.path.join(saving_path, dir)
        if not os.path.exists(using_part):
            os.makedirs(using_part)
        for class_dir in class_names:
            class_path = os.path.join(using_part, class_dir)
            if not os.path.exists(class_path):
                os.makedirs(class_path)

    total_patients = len(patients_imaging_data)

    training_part = int(floor(total_patients*list_of_cuts[0]))
    validation_part = int(floor(total_patients*list_of_cuts[1]))
    testing_part = int(floor(total_patients*list_of_cuts[2]))

    parts = [training_part, validation_part ,testing_part]
    class_names = ["ER+", "ER-"]
    start, finish = 0, 0
    for p in range(3):
        start = finish
        finish += parts[p]
        for patient_number in range(start, finish):
            for image in patients_imaging_data[patient_number]:
                if patients_images_labels[patient_number][patients_imaging_data[patient_number].index(image)] != None:
                    img = Image.open(os.path.join(input_dir ,image.replace('\\', '/')))

                    # unscaling a little bit the images so i could cut them into the 16 image tiles, therefor,
                    # inlarging the dataset and keeping all the images with high resolution
                    img = img.resize((1196,1196), Image.Resampling.LANCZOS)
                    # Get the dimensions of the original image
                    width, height = img.size

                    # Define the number of rows and columns for your grid (4x4 for 16 tiles)
                    rows, columns = 4, 4

                    # Calculate the width and height of each tile
                    tile_width = width // columns
                    tile_height = height // rows

                    # Loop through the rows and columns and crop the image into tiles
                    counter = 0
                    for i in range(rows):
                        for j in range(columns):
                            left = j * tile_width
                            upper = i * tile_height
                            right = left + tile_width
                            lower = upper + tile_height

                            # Crop the tile
                            tile = img.crop((left, upper, right, lower))

                            # Apply sharpening
                            tile = sharpen_image(tile)
                            

                            #NOT IN USE NOW:

                            #apply mask to clear the image from unusfull information
                            # Convert the image to grayscale
                            gray_image = color.rgb2gray(tile)
                            # Create a binary mask where 0 represents the background
                            threshold = filters.threshold_otsu(gray_image)
                            background_mask = gray_image > threshold
                            # Set the background to black
                            tile = np.array(tile)  # Ensure 'tile' is a NumPy array
                            tile[background_mask, :] = [0, 0, 0]
                            tile = Image.fromarray(tile)




                            if tile.mode == 'RGBA':
                                tile = tile.convert('RGB', dither=Image.NONE, palette=Image.ADAPTIVE)
                            if tile.mode == 'P':
                                tile = tile.convert('RGB', dither=Image.NONE, palette=Image.ADAPTIVE)

                            if patients_images_labels[patient_number][patients_imaging_data[patient_number].index(image)] == 1:
                                save_path = os.path.join(saving_path, parts_path_name[p], class_names[1], os.path.basename(image.replace('.jpg','')) + str(counter)+ '.jpg')
                                tile.save(save_path)
                                ERnegative_class_objects.append(os.path.join(saving_path, parts_path_name[p], class_names[1], os.path.basename(image.replace('.jpg','')) + str(counter)+ '.jpg')) #getting all the negative images
                                ERnegative_class_object_counter += 1 #getting the number of negative image
                            else:
                                save_path = os.path.join(saving_path, parts_path_name[p], class_names[0], os.path.basename(image.replace('.jpg','')) + str(counter)+ '.jpg')
                                tile.save(save_path)
                                ERpositive_class_objects.append(os.path.join(saving_path, parts_path_name[p], class_names[0], os.path.basename(image.replace('.jpg','')) + str(counter)+ '.jpg')) #getting all the positive images
                                ERpositive_class_object_counter += 1 #getting the number of positive image

                            counter +=1

    return (ERpositive_class_object_counter, ERnegative_class_object_counter, ERpositive_class_objects, ERnegative_class_objects)

def sharpen_image(image):
    # Convert PIL Image to NumPy array
    image_np = np.array(image)

    # Apply unsharp mask for sharpening
    sharpened_image = filters.unsharp_mask(image_np, radius=1, amount=1)

    # Convert the NumPy array back to a PIL Image
    sharpened_image = Image.fromarray((sharpened_image * 255).astype(np.uint8))

    return sharpened_image

saving_path = "/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/model_usage_data"

ERpositive_image_saving_path = "/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/model_usage_data/ERpositive_image_saving_path"
ERnegative_image_saving_path = "/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/model_usage_data/ERnegative_image_saving_path"
ERPC, ERNC, ERpositive_class_objects, ERnegative_class_objects = folder_sorting([0.7,0.15,0.15], saving_path)

ready_information_path = "/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/model_usage_data"
#filtering all the unusfull subtile parts
parts_path_name = ["training_part", "validation_part", "testing_part"]
for i in range(3):
    set = parts_path_name[i]
    for workingSet in os.listdir(os.path.join(ready_information_path, set)):
        for image in os.listdir(os.path.join(ready_information_path, set, workingSet)):
            # Load the image using PIL
            opened_image = Image.open(os.path.join(ready_information_path, set, workingSet, image))

            # Convert the image to a NumPy array
            image_array = np.array(opened_image)

            # Define the lower and upper bounds for the color range
            lower_bound = np.array([230, 230, 230], dtype=np.uint8)
            upper_bound = np.array([255, 255, 255], dtype=np.uint8)

            # Create a mask that identifies pixels within the specified color range
            mask = np.all((image_array >= lower_bound) & (image_array <= upper_bound), axis=-1)

            # Calculate the percentage of pixels within the specified color range
            total_pixels = mask.size
            matching_pixels = np.count_nonzero(mask)
            percentage = (matching_pixels / total_pixels) * 100

            if percentage > 70:
                os.remove(os.path.join(ready_information_path, set, workingSet, image))
                if os.path.join(ready_information_path, set, workingSet, image) in ERpositive_class_objects:
                    ERpositive_class_objects.remove(os.path.join(ready_information_path, set, workingSet, image))
                if os.path.join(ready_information_path, set, workingSet, image) in ERnegative_class_objects:
                    ERnegative_class_objects.remove(os.path.join(ready_information_path, set, workingSet, image))



#making directories to include all the images from each class for later usage
if not os.path.exists(ERpositive_image_saving_path):
    os.makedirs(ERpositive_image_saving_path)
if not os.path.exists(ERnegative_image_saving_path):
    os.makedirs(ERnegative_image_saving_path)

for image in ERpositive_class_objects:
    img = Image.open(image)
    save_path = os.path.join(ERpositive_image_saving_path, os.path.basename(image))
    img.save(save_path)

for image in ERnegative_class_objects:
    img = Image.open(image)
    save_path = os.path.join(ERnegative_image_saving_path, os.path.basename(image))
    img.save(save_path)

#balacing the data set classes
parts_path_name = ["training_part/ER-", "validation_part/ER-", "testing_part/ER-"]
new_images_needed = ERPC - ERNC
number_of_augmentation_sycels_needed = int(new_images_needed / ERNC) #calculation how many times i need to the the augmentation on the class that i want in order to have a balanced number of images in each class
number_per_packeg = [int(floor(number_of_augmentation_sycels_needed*0.7)),int(floor(number_of_augmentation_sycels_needed*0.15)),int(floor(number_of_augmentation_sycels_needed*0.15))]
original_data_path = ERnegative_image_saving_path

#creating the augmentor to balnce the data by generating more of the samll class
for i in range(3):
    current_output_directory = os.path.join(saving_path, parts_path_name[i])

    p = Augmentor.Pipeline(original_data_path, current_output_directory)
    p.rotate(probability=0.7, max_left_rotation=25, max_right_rotation=25)
    p.flip_left_right(probability=0.5)
    p.zoom_random(probability=0.5, percentage_area=0.9)

    for j in range(number_per_packeg[i]):
        p.sample(ERNC) #calling a new image to be made from the dataset 1059 times, so all the images will get there own edited version
'''

batch_size = 10
train_datagen = ImageDataGenerator(
    rotation_range=180,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    vertical_flip=True,
    zoom_range=0.1,
    rescale=1.0 / 255,
)

# Define the data generator for training data
train_generator = train_datagen.flow_from_directory(
    "/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/model_usage_data/training_part",
    target_size=(299, 299),
    batch_size=batch_size,
    class_mode='binary',
    shuffle=True
)

# Define a separate data generator for validation data (without data augmentation)
validation_datagen = ImageDataGenerator(
    rotation_range=180,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    vertical_flip=True,
    zoom_range=0.1,
    rescale=1.0 / 255,
)

validation_generator = validation_datagen.flow_from_directory(
    "/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/model_usage_data/validation_part",
    target_size=(299, 299),
    batch_size=batch_size,
    class_mode='binary',
    shuffle=True
)

# Define a separate data generator for the test data (including rescaling)
test_datagen = ImageDataGenerator(
    rescale=1.0 / 255,  # Apply the same rescaling as in training and validation
)

test_generator = test_datagen.flow_from_directory(
    "/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/model_usage_data/testing_part",
    target_size=(299, 299),
    batch_size=batch_size,
    class_mode='binary',
    shuffle=True
)

class MultiHeadAttentionLayer(tf.keras.layers.Layer):
    def __init__(self, num_heads=8, head_size=512, dropout=0.1):
        super(MultiHeadAttentionLayer, self).__init__()
        self.num_heads = num_heads
        self.head_size = head_size
        self.dropout = dropout

    def build(self, input_shape):
        self.spatial_query_dense = Conv2D(self.num_heads * self.head_size, kernel_size=1, padding='same')
        self.spatial_key_dense = Conv2D(self.num_heads * self.head_size, kernel_size=1, padding='same')
        self.spatial_value_dense = Conv2D(self.num_heads * self.head_size, kernel_size=1, padding='same')

        self.channel_query_dense = Dense(self.num_heads * self.head_size)
        self.channel_key_dense = Dense(self.num_heads * self.head_size)
        self.channel_value_dense = Dense(self.num_heads * self.head_size)

        self.dropout_layer = Dropout(self.dropout)
        self.concat_layer = Concatenate(axis=-1)
        self.output_dense = Conv2D(self.head_size, kernel_size=1, padding='same')

    def call(self, inputs):
        # Spatial Attention
        spatial_q = self.spatial_query_dense(inputs)
        spatial_k = self.spatial_key_dense(inputs)
        spatial_v = self.spatial_value_dense(inputs)

        spatial_q = self.split_heads(spatial_q)
        spatial_k = self.split_heads(spatial_k)
        spatial_v = self.split_heads(spatial_v)

        spatial_attention, spatial_weights = self.scaled_dot_product_attention(spatial_q, spatial_k, spatial_v)

        # Channel Attention
        channel_q = self.channel_query_dense(inputs)
        channel_k = self.channel_key_dense(inputs)
        channel_v = self.channel_value_dense(inputs)

        channel_q = self.split_heads(channel_q, axis=-1)
        channel_k = self.split_heads(channel_k, axis=-1)
        channel_v = self.split_heads(channel_v, axis=-1)

        channel_attention, channel_weights = self.scaled_dot_product_attention(channel_q, channel_k, channel_v, axis=-2)

        # Combine Spatial and Channel Attention
        attention = self.concat_layer([spatial_attention, channel_attention])
        attention = self.output_dense(attention)

        return attention

    def split_heads(self, x, axis=-1):
        batch_size = tf.shape(x)[0]
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_size))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def scaled_dot_product_attention(self, q, k, v, axis=-1):
        matmul_qk = tf.matmul(q, k, transpose_b=True)
        dk = tf.cast(tf.shape(k)[-1], tf.float32)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=axis)
        attention_weights = self.dropout_layer(attention_weights)

        output = tf.matmul(attention_weights, v)

        return output, attention_weights


# In your main function
def build_model_with_attention(turn):
    base_model = keras.applications.Xception(
        include_top=False,
        weights='imagenet',
        input_shape=(299, 299, 3),
        pooling="avg"
    )

    # Unfreeze the layers in the exit flow
    exit_flow_layers = ['block13_sepconv2', 'block14_sepconv1', 'block14_sepconv2']
    for layer in base_model.layers:
        if any(exit_layer in layer.name for exit_layer in exit_flow_layers):
            layer.trainable = True

    # Create multi-head attention layer
    attention_layer = MultiHeadAttentionLayer()

    # Reshape the output to have the required 4D shape
    reshaped_output = Reshape((1, 1, 2048))(base_model.output)

    # Apply the attention layer to the output of the base model
    attention_output = attention_layer(reshaped_output)

    # Apply global average pooling to the attention output
    pooled_output = GlobalAveragePooling2D()(attention_output)

    # Apply dropout to the pooled output
    dropped_output = Dropout(0.2)(pooled_output)

    # Add a new final layer for binary classification with L2 regularization
    outputs = Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(dropped_output)

    if turn == 1:
        model = Model(inputs=base_model.input, outputs=outputs)
    elif turn == 2:
        selected_layer = 'block14_sepconv2_act'
        model = Model(inputs=base_model.input, outputs=base_model.get_layer(selected_layer).output)
    else:
        raise ValueError("Invalid value for 'turn'. Please use 1 or 2.")

    # Compile the model
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
        loss='binary_crossentropy',
        metrics=['binary_accuracy']
    )

    return model

# Build and train the model with attention
model_with_attention = build_model_with_attention(1)

#building the model hyperparameters by hand
#model = build_model()

# Display the model summary to verify the architecture
#model.summary()
model_with_attention.summary()

# Define the early stopping callback
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)

#mking castume class whiths to try and change the way the model learn the negtive class att least for now
#class_weights = {0: 1, 1: 1.5}

# Fit the model using the data generators
tf.keras.backend.clear_session()
epochs = 5  # Adjust the number of epochs as needed
history = model_with_attention.fit(
    train_generator,
    steps_per_epoch=len(train_generator)/10,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)/10,
    callbacks=[early_stopping],
    use_multiprocessing=True,
    workers=4  # Adjust based on your GPU cores
)

def generate_activation_and_saliency_maps(model, image_path, hm_saving_path, saliency_maps_saving_path, layer_name='dense'):
    input_img = np.array(Image.open(image_path))
    input_img = np.expand_dims(input_img, axis=0)
    
    y_pred = model.predict(input_img)
    images = tf.Variable(input_img, dtype=float)

    with tf.GradientTape() as tape:
        pred = model(images, training=False)
        class_idxs_sorted = np.argsort(pred.numpy().flatten())[::-1]
        loss = pred[0][class_idxs_sorted[0]]
    
    grads = tape.gradient(loss, images)
    dgrad_abs = tf.math.abs(grads)
    dgrad_max_ = np.max(dgrad_abs, axis=3)[0]
    arr_min, arr_max  = np.min(dgrad_max_), np.max(dgrad_max_)
    grad_eval = (dgrad_max_ - arr_min) / (arr_max - arr_min + 1e-18)

    # Create a directory to save saliency maps
    if not os.path.exists(saliency_maps_saving_path):
        os.makedirs(saliency_maps_saving_path)

    # Save the heatmap
    plt.imsave(os.path.join(saliency_maps_saving_path, 'saliency_map.png'), grad_eval, cmap='jet')

# Generate activation maps for an example image
example_image_path = '/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/model_usage_data/training_part/ER+/Patient_1_2011Subregion_01_2011_0_2_412.jpg'
activation_maps_saving_path = '/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/model_usage_data/heatmaps_and_more'
silency_maps_saving_path = '/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/model_usage_data/silency_maps_and_more'
generate_activation_and_saliency_maps(model_with_attention, example_image_path, activation_maps_saving_path, silency_maps_saving_path)

# Assuming you have true labels and predicted labels
true_labels =  test_generator.classes
y_pred = model_with_attention.predict(test_generator)
predicted_labels = (y_pred > 0.1).astype(int)

# Calculate F1 Score
f1 = f1_score(true_labels, predicted_labels)
print("F1 Score:", f1)

# Calculate AUC-ROC
fpr, tpr, thresholds = roc_curve(true_labels, predicted_labels)
auc_roc = auc(fpr, tpr)
print("AUC-ROC:", auc_roc)

# Generate the confusion matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)
print("Confusion Matrix:")
print(conf_matrix)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc)
plt.plot([0.0, 1.0], [0.0, 1.0], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show(block=True)

model_with_attention = build_model_with_attention(2)

# Set the path to your training images directory
train_images_directory = '/mnt/c/Users/owner/OneDrive/שולחן העבודה/archive (1)/IHC4BC_Compressed/Images/HandE/model_usage_data/all images'

# List all files in the directory
image_files = [f for f in os.listdir(train_images_directory) if os.path.isfile(os.path.join(train_images_directory, f))]

# Container for extracted features
all_features = []

# Iterate over image files
for image_file in image_files:
    # Construct the full path to the image
    img_path = os.path.join(train_images_directory, image_file)

    # Open and preprocess the image using Pillow
    img = Image.open(img_path)
    img = img.resize((299, 299))  # Resize to the input size expected by Xception
    img_array = keras.preprocessing.image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = keras.applications.xception.preprocess_input(img_array)

    # Extract features using predict()
    features = model_with_attention.predict(img_array)
    print(features)

    # Append the features to the list
    all_features.append(features)

# Convert the list to a numpy array
all_features = np.vstack(all_features)

# 'all_features' now contains the extracted features for all images in the training set
print(all_features.shape)
